diff --git a/src/gpu/jit/gemm/gemm_recipes.hpp b/src/gpu/jit/gemm/gemm_recipes.hpp
index 6f02eb111..68f7407fb 100644
--- a/src/gpu/jit/gemm/gemm_recipes.hpp
+++ b/src/gpu/jit/gemm/gemm_recipes.hpp
@@ -94,6 +94,12 @@ const gemm_recipe_t gemm_recipes[] = {
     {ngen::HW::Gen12LP, "HHH", "TNN", {}, 32, 32, "as4 as8 ab k8 ra4 l4 vnc"},
     {ngen::HW::Gen12LP, "HHH", "TTN", {}, 32, 16, "as8 ab4x2 ab k16 ra8 l4 int"},
     {ngen::HW::Gen12LP, "HHH", "TTN", {}, 32, 32, "as8 ab2x2 ab k16 ra8 l4 vnc"},
+    {ngen::HW::Gen12HP, "SSS", "NNN", {}, 64, 8,  "sb1x2 su4/2x2 sb ca1 wg 2x8 cs di bm8192 bn8192 bk8192"},
+    {ngen::HW::Gen12HP, "SSS", "NNN", {}, 32, 8,  "sb4 sb8 sb cab1 wg 4x4 cs di bm4096 bn4096 bk8192"},
+    {ngen::HW::Gen12HP, "SSS", "NNN", {}, 16, 8,  "sb16 sb16 sb cab1 wg 4x4 cs di bm2048 bn2048 bk8192"},
+    {ngen::HW::Gen12HP, "SSS", "NTN", {}, 32, 16, "sb4/2x2 sb4x2 sb cs di bm8192 bn8192 bk8192"},
+    {ngen::HW::Gen12HP, "SSS", "TNN", {}, 16, 16, "sb8 sb8 su cab1 wg 4x4 cs di bm8192 bn8192 bk8192"},
+    {ngen::HW::Gen12HP, "SSS", "TTN", {}, 8,  64, "su4/2x2 sb1x2 su cb1 wg 8x2 cs di fn bm8192 bn8192 bk8192"},
     {ngen::HW::Gen12HP, "HHH", "NNN", {}, 32, 32, "ab4/1x2 as4/1x2 ab l4 cb1 wg 8x2 cs nmk"},
     {ngen::HW::Gen12HP, "HHH", "NTN", {}, 32, 32, "ab4/1x2 ab2/1x2 ab l4 cs"},
     {ngen::HW::Gen12HP, "HHH", "TNN", {}, 16, 16, "as8 as32 ab l4 cab1 wg 4x4 cs"},
diff --git a/src/gpu/jit/gemm/gen_gemm.hpp b/src/gpu/jit/gemm/gen_gemm.hpp
index b56e4b44e..1cb777e25 100644
--- a/src/gpu/jit/gemm/gen_gemm.hpp
+++ b/src/gpu/jit/gemm/gen_gemm.hpp
@@ -57,11 +57,9 @@ struct gen_gemm_t : public gpu_gemm_t {
 
             // LIMITATIONS:
             // - runtime dims are not supported
-            // - bias is not supported
             bool limits_ok = true
                     && !utils::one_of(DNNL_RUNTIME_DIM_VAL, d->m, d->n, d->k,
-                            d->lda, d->ldb, d->ldc, d->batch)
-                    && d->bias_type == data_type::undef;
+                            d->lda, d->ldb, d->ldc, d->batch);
 
             bool ok = limits_ok && utils::one_of(d->c_type, f32, f16)
                     && d->a_type == d->c_type && d->b_type == d->c_type
@@ -71,7 +69,10 @@ struct gen_gemm_t : public gpu_gemm_t {
                     && attr()->output_scales_.mask_ == 0
                     && attr()->post_ops_.len() <= 1
                     && IMPLICATION(attr()->post_ops_.len() == 1,
-                            attr()->post_ops_.find(sum) != -1);
+                            attr()->post_ops_.find(sum) != -1)
+                    && IMPLICATION(with_bias(),
+                            (d->bias_type == d->c_type)
+                                    && utils::one_of(bias_cmask(), 0, 1 << 0, 1 << 1));
 
             if (!ok) return status::unimplemented;
 
@@ -82,9 +83,6 @@ struct gen_gemm_t : public gpu_gemm_t {
             ok &= utils::one_of(
                     arch_, arch_t::gen9, arch_t::gen12lp, arch_t::gen12hp);
 
-            // Only f16 enabled on Gen12HP for now.
-            ok &= IMPLICATION(arch_ == arch_t::gen12hp, d->c_type == f16);
-
             if (!ok) return status::unimplemented;
 
             eu_count_ = dev_info->eu_count();
@@ -111,6 +109,13 @@ struct gen_gemm_t : public gpu_gemm_t {
             return p.contain(sum, 0) ? p.entry_[0].sum.scale : 0.f;
         }
 
+        bool with_bias() const { return desc()->bias_type != data_type::undef; }
+
+        int bias_cmask() const {
+            unsigned char to_cmask[4] = {0, 2, 1, 3};
+            return with_bias() ? to_cmask[(desc()->bias_mask >> 1) & 3] : -1;
+        }
+
         size_t dyn_offset_a = 0;
         size_t dyn_offset_b = 0;
         size_t dyn_offset_c = 0;
@@ -143,7 +148,7 @@ struct gen_gemm_t : public gpu_gemm_t {
 
         kernel_t kernel;
 
-        auto status = kernel.init(pd()->arch_, batched, transa, transb, a_type,
+        auto status = kernel.init(pd()->arch_, batched, transa, transb, pd()->with_bias(),
                 b_type, c_type, unroll_m, unroll_n);
         if (status != status::success) return status;
 
diff --git a/src/gpu/jit/gemm/gen_gemm_kernel.hpp b/src/gpu/jit/gemm/gen_gemm_kernel.hpp
index 95eecc2db..0e1755143 100644
--- a/src/gpu/jit/gemm/gen_gemm_kernel.hpp
+++ b/src/gpu/jit/gemm/gen_gemm_kernel.hpp
@@ -83,7 +83,7 @@ private:
 
 struct gen_gemm_nocopy_kernel_t : public gen_gemm_kernel_t {
     status_t init(compute::gpu_arch_t arch, bool batch, bool trans_a,
-            bool trans_b, data_type_t a_type, data_type_t b_type,
+            bool trans_b, bool bias, data_type_t a_type, data_type_t b_type,
             data_type_t c_type, int unroll_m, int unroll_n) {
 
         problem_.Ta = convert_dnnl_to_kernel_type(a_type);
@@ -102,6 +102,7 @@ struct gen_gemm_nocopy_kernel_t : public gen_gemm_kernel_t {
         problem_.B.base = ngen::AddressBase::createA64(true);
         problem_.C.base = ngen::AddressBase::createA64(true);
         problem_.batchedS = batch;
+        problem_.cOffset = bias ? COffset::Pre : COffset::None;
 
         strategy_.unroll[LoopM] = unroll_m;
         strategy_.unroll[LoopN] = unroll_n;
